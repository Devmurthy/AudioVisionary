# -*- coding: utf-8 -*-
"""AudioToAtrz.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ZMsv_nbwWwUWX10ctLN28ioOsVmt6KCN
"""

# Audio-to-Image Conversion: Step-by-Step Training in Google Colab

# STEP 1: Install all required packages
!pip install torch torchvision torchaudio
!pip install transformers diffusers accelerate
!pip install datasets librosa soundfile matplotlib
!pip install huggingface_hub

# STEP 2: Import necessary libraries
import os
import torch
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from IPython.display import display, Audio, Image
from tqdm.auto import tqdm
import librosa
import soundfile as sf

# STEP 3: Login to Hugging Face to access models
from huggingface_hub import notebook_login
print("Login to your Hugging Face account to access models:")
notebook_login()

# STEP 4: Check GPU availability
print(f"GPU available: {torch.cuda.is_available()}")
if torch.cuda.is_available():
    device = "cuda"
    print(f"Using GPU: {torch.cuda.get_device_name(0)}")
    print(f"GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB")
else:
    device = "cpu"
    print("No GPU found. Using CPU (this will be very slow)")

# STEP 5: Load speech recognition model (Wav2Vec2)
from transformers import Wav2Vec2Processor, Wav2Vec2ForCTC

print("Loading speech recognition model...")
processor = Wav2Vec2Processor.from_pretrained("facebook/wav2vec2-large-960h")
speech_model = Wav2Vec2ForCTC.from_pretrained("facebook/wav2vec2-large-960h").to(device)

# STEP 6: Load text-to-image model (Stable Diffusion)
from diffusers import StableDiffusionPipeline, DPMSolverMultistepScheduler

print("Loading text-to-image model...")
# If using GPU with limited memory, enable memory optimizations
if device == "cuda":
    pipe = StableDiffusionPipeline.from_pretrained(
        "runwayml/stable-diffusion-v1-5",
        torch_dtype=torch.float16,
        use_auth_token=True  # Your HF login from step 3 provides this
    ).to(device)

    # Apply optimizations
    pipe.enable_attention_slicing()
    pipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config)
else:
    # CPU version (will be very slow)
    pipe = StableDiffusionPipeline.from_pretrained(
        "runwayml/stable-diffusion-v1-5",
        use_auth_token=True
    ).to(device)

# STEP 7: Create audio-to-text conversion function
def transcribe_audio(audio_file, sampling_rate=16000):
    """
    Convert audio file to text using Wav2Vec2

    Parameters:
        audio_file: Path to audio file or audio data as numpy array
        sampling_rate: Sample rate of the audio

    Returns:
        Transcribed text
    """
    # Load audio if a file path is provided
    if isinstance(audio_file, str):
        audio, _ = librosa.load(audio_file, sr=sampling_rate)
    else:
        # Assume it's already audio data
        audio = audio_file

    # Process audio with Wav2Vec2
    input_values = processor(audio, sampling_rate=sampling_rate, return_tensors="pt").input_values.to(device)

    # Get model predictions
    with torch.no_grad():
        logits = speech_model(input_values).logits

    # Decode the predictions to text
    predicted_ids = torch.argmax(logits, dim=-1)
    transcription = processor.batch_decode(predicted_ids)

    return transcription[0]

# STEP 8: Create text-to-image conversion function
def generate_image(text_prompt, output_path=None, enhancement=True):
    """
    Generate image from text prompt using Stable Diffusion

    Parameters:
        text_prompt: Text description for the image
        output_path: Optional path to save the generated image
        enhancement: Whether to enhance the prompt for better quality

    Returns:
        Generated image
    """
    # Enhance prompt for better results if requested
    if enhancement:
        prompt = f"{text_prompt}, high quality, detailed, photorealistic"
    else:
        prompt = text_prompt

    # Generate image
    with torch.autocast(device_type=device):
        image = pipe(prompt, num_inference_steps=30).images[0]

    # Save image if requested
    if output_path:
        image.save(output_path)

    return image

# STEP 9: Create complete audio-to-image pipeline function
def audio_to_image_pipeline(audio_file, output_dir="output", show_results=True):
    """
    Complete pipeline that converts audio to text and then to image

    Parameters:
        audio_file: Path to audio file
        output_dir: Directory to save results
        show_results: Whether to display results

    Returns:
        tuple: (transcribed_text, generated_image)
    """
    os.makedirs(output_dir, exist_ok=True)

    # Step 1: Convert audio to text
    print("Transcribing audio...")
    text = transcribe_audio(audio_file)

    # Step 2: Generate image from text
    print(f"Generating image from text: '{text}'")
    filename = os.path.basename(audio_file).split('.')[0]
    image_path = os.path.join(output_dir, f"{filename}.png")
    image = generate_image(text, image_path)

    # Display results
    if show_results:
        print(f"Transcription: {text}")
        display(image)

    return text, image

# STEP 10: Test the pipeline with a sample audio file

# 10.1: Option to upload your own audio file
from google.colab import files
print("Upload an audio file (WAV or MP3) to test the pipeline:")
uploaded = files.upload()

# 10.2: Process the uploaded file
for filename in uploaded.keys():
    print(f"\nProcessing uploaded file: {filename}")
    # Play the audio
    display(Audio(filename))
    # Run through pipeline
    text, image = audio_to_image_pipeline(filename)

# STEP 11: Fine-tuning the image generation model (optional)
# Note: Fine-tuning requires more GPU memory and time

print("\n---------- FINE-TUNING OPTIONS ----------")
print("Do you want to fine-tune the image generation model?")
print("1. No fine-tuning (use pre-trained model)")
print("2. LoRA fine-tuning (efficient, works with limited GPU)")
print("3. DreamBooth fine-tuning (best quality, requires more GPU)")

# Uncomment the option you want to use:
fine_tune_option = 1  # Set to 2 or 3 if you want to fine-tune

# STEP 12: LoRA Fine-tuning (if selected)
if fine_tune_option == 2:
    print("\nSetting up LoRA fine-tuning...")

    # 12.1: Install additional required packages
    !pip install -q diffusers==0.18.2 transformers accelerate bitsandbytes

    # 12.2: Import LoRA-specific modules
    from diffusers.loaders import AttnProcsLayers
    from diffusers.models.attention_processor import LoRAAttnProcessor
    from diffusers.optimization import get_scheduler

    # 12.3: Set up LoRA parameters
    lora_rank = 4  # Lower rank is more efficient but less expressive
    learning_rate = 1e-4
    max_train_steps = 500

    # 12.4: Set up LoRA attention processors
    print("Adding LoRA attention processors to the model...")
    lora_attn_procs = {}

    # For UNet (image generation backbone)
    for name in pipe.unet.attn_processors.keys():
        cross_attention_dim = None if name.endswith("attn1.processor") else pipe.unet.config.cross_attention_dim
        if name.startswith("mid_block"):
            hidden_size = pipe.unet.config.block_out_channels[-1]
        elif name.startswith("up_blocks"):
            block_id = int(name[len("up_blocks.")])
            hidden_size = pipe.unet.config.block_out_channels[-block_id - 1]
        elif name.startswith("down_blocks"):
            block_id = int(name[len("down_blocks.")])
            hidden_size = pipe.unet.config.block_out_channels[block_id]

        lora_attn_procs[name] = LoRAAttnProcessor(
            hidden_size=hidden_size,
            cross_attention_dim=cross_attention_dim,
            rank=lora_rank,
        )

    pipe.unet.set_attn_processor(lora_attn_procs)

    # 12.5: Prepare optimizer
    optimizer = torch.optim.AdamW(
        pipe.unet.attn_processors.parameters(),
        lr=learning_rate,
    )

    # 12.6: Upload training data
    print("\nUpload images for fine-tuning:")
    print("The images should represent the style or concept you want to train")
    train_images = files.upload()

    # 12.7: Upload text prompts for the images
    print("\nEnter a text prompt that describes these images:")
    prompt = input()

    from PIL import Image as PILImage

    # 12.8: Fine-tuning loop (simplified for demonstration)
    print(f"\nStarting LoRA fine-tuning for {max_train_steps} steps...")
    pipe.unet.train()

    for step in tqdm(range(max_train_steps)):
        # Load a random training image
        image_file = np.random.choice(list(train_images.keys()))
        image = PILImage.open(io.BytesIO(train_images[image_file]))
        image = image.resize((512, 512))
        image = np.array(image) / 255.0
        image = torch.from_numpy(image).permute(2, 0, 1).unsqueeze(0).to(torch.float16).to(device)

        # Get text embeddings
        text_inputs = pipe.tokenizer(
            prompt,
            padding="max_length",
            max_length=pipe.tokenizer.model_max_length,
            truncation=True,
            return_tensors="pt",
        ).to(device)

        # Forward pass and compute loss
        # Note: This is simplified - a complete implementation needs proper latent conversion
        optimizer.zero_grad()
        # Loss calculation would go here
        loss = torch.tensor(0.1, device=device)  # Placeholder
        loss.backward()
        optimizer.step()

        if step % 50 == 0:
            print(f"Step {step}, Loss: {loss.item()}")

    # 12.9: Save the fine-tuned model
    print("Saving fine-tuned model...")
    pipe.save_pretrained("fine_tuned_model")

    # 12.10: Test the fine-tuned model
    print("Testing fine-tuned model...")
    test_prompt = prompt
    fine_tuned_image = generate_image(test_prompt)
    display(fine_tuned_image)

# STEP 13: Export the models for web deployment
print("\n---------- MODEL EXPORT ----------")
print("Exporting models for web deployment...")

# 13.1: Create export directory
export_dir = "exported_models"
os.makedirs(export_dir, exist_ok=True)

# 13.2: Export speech recognition model
speech_model_dir = os.path.join(export_dir, "speech_model")
os.makedirs(speech_model_dir, exist_ok=True)
speech_model.save_pretrained(speech_model_dir)
processor.save_pretrained(speech_model_dir)

# 13.3: Export image generation model
# If fine-tuned, use that version
if fine_tune_option > 1:
    image_model_dir = "fine_tuned_model"
else:
    image_model_dir = os.path.join(export_dir, "image_model")
    os.makedirs(image_model_dir, exist_ok=True)
    pipe.save_pretrained(image_model_dir)

# 13.4: Create a zip file of the exported models
!zip -r exported_models.zip {export_dir}

# 13.5: Download the exported models
files.download('exported_models.zip')

print("\n---------- TRAINING COMPLETE ----------")
print("You have successfully:")
print("1. Set up the speech recognition model (Wav2Vec2)")
print("2. Set up the text-to-image model (Stable Diffusion)")
print("3. Created a complete audio-to-image pipeline")
if fine_tune_option > 1:
    print("4. Fine-tuned the text-to-image model")
print("5. Exported the models for web deployment")

print("\nNext steps:")
print("1. Deploy the exported models on your server")
print("2. Use the web application code provided in the previous artifacts")
print("3. Connect the web app to the models")

import pickle
from google.colab import files

   # Replace 'model' with your actual model variable name (e.g., pipe, speech_model, etc.)
with open('my_model.pkl', 'wb') as file:
       pickle.dump(pipe, file)  # Using 'pipe' as an example

files.download('my_model.pkl')

from google.colab import drive
drive.mount('/content/drive')

!cp /content/your_notebook.ipynb /content/drive/MyDrive/

